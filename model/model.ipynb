{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c370820d-ed7e-494a-bfdc-06c3266ef940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c68b1-cc93-4d52-bd24-b9875cfed1a2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preprocessing of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e4d007-427b-4bbe-abe8-437d9cba5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaedf56f-1510-4db5-b59c-5b0f69bfb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the  base64 img from file\n",
    "\n",
    "def load_base64_img():\n",
    "    with open('img.txt') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c6b4ed-e265-455f-b4a5-1ffd0e73cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the base64 image/ image path --> transparent image to white background ---> to array using cv2\n",
    "\n",
    "def get_cv2_image_from_base64_string(b64str, img_path):\n",
    "    if img_path:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    else:\n",
    "        encoded_data = b64str.split(',')[1]\n",
    "        nparr = np.frombuffer(base64.b64decode(encoded_data), np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED) # load the image with alpha channel\n",
    "        \n",
    "    \n",
    "    ## transparent to white bg \n",
    "    alpha_channel = img[: ,: , 3]\n",
    "    _, mask = cv2.threshold(alpha_channel, 254, 255, cv2.THRESH_BINARY) # binarize mask\n",
    "    color = img[: ,: ,: 3]\n",
    "    new_img = cv2.bitwise_not(cv2.bitwise_not(color, mask = mask))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea681352-e7b4-4725-8af7-7066a3df9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing the image into 28x28 pixels\n",
    "\n",
    "def get_img_reshape_by_cv2(img_data):\n",
    "    image = img_data\n",
    "    grey = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY) # convert to gray\n",
    "    ret, thresh = cv2.threshold(grey.copy(), 75, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    preprocessed_digits = []\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "\n",
    "        # Creating a rectangle around the digit in the original image (for displaying the digits fetched via contours)\n",
    "        roi = cv2.rectangle(image, (x,y), (x+w, y+h), color=(0, 255, 0), thickness=1)\n",
    "\n",
    "        # Cropping out the digit from the image corresponding to the current contours in the for loop\n",
    "        digit = thresh[y:y+h, x:x+w]\n",
    "\n",
    "        # Resizing that digit to (18, 18)\n",
    "        resized_digit = cv2.resize(digit, (500, 500))\n",
    "        # resized_digit = cv2.resize(resized_digit, (500,500))\n",
    "        resized_digit = cv2.resize(resized_digit, (400,400))\n",
    "        resized_digit = cv2.resize(resized_digit, (300,300))\n",
    "        resized_digit = cv2.resize(resized_digit, (200,200))\n",
    "        resized_digit = cv2.resize(resized_digit, (100,100))\n",
    "        \n",
    "        resized_digit = cv2.resize(resized_digit, (50,50))\n",
    "        resized_digit = cv2.resize(resized_digit, (40,40))\n",
    "        resized_digit = cv2.resize(resized_digit, (25,25))\n",
    "        resized_digit = cv2.resize(resized_digit, (18,18)) ##### <------\n",
    "\n",
    "        # Padding the digit with 5 pixels of black color (zeros) in each side to finally produce the image of (28, 28)\n",
    "        padded_digit = np.pad(resized_digit, ((5,5),(5,5)), \"constant\", constant_values=0)\n",
    "\n",
    "        # plt.imshow(padded_digit)\n",
    "        \n",
    "        \n",
    "        # Adding the preprocessed digit to the list of preprocessed digits\n",
    "        preprocessed_digits.append(padded_digit)\n",
    "\n",
    "    # plt.imshow(padded_digit, cmap=\"gray\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # print(preprocessed_digits)\n",
    "    inp = np.array(preprocessed_digits)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633a2f0-31ae-4604-8fab-5a3fc1da2bae",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preprocessing of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7314a6b-a427-455d-b088-d4cb7691748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path of dataset and cropped img\n",
    "\n",
    "path_to_data = \"./dataset/\"\n",
    "path_to_cropped = \"./dataset/cropped/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8d6988-0873-4bef-8f04-452f82fde51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "if os.path.exists(path_to_cropped):\n",
    "    shutil.rmtree(path_to_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55609c5b-ea19-49d8-abd9-eeff41ee79ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/0',\n",
       " './dataset/1',\n",
       " './dataset/2',\n",
       " './dataset/3',\n",
       " './dataset/4',\n",
       " './dataset/5',\n",
       " './dataset/6',\n",
       " './dataset/7',\n",
       " './dataset/8',\n",
       " './dataset/9']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the digits image dir from the dataset into the list\n",
    "\n",
    "import os\n",
    "img_dirs = []\n",
    "\n",
    "for entry in os.scandir(path_to_data):\n",
    "    if entry.is_dir():\n",
    "        img_dirs.append(entry.path)\n",
    "\n",
    "img_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da984bea-e4cd-415d-a89a-345b6168199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the cropped img folder is present or not\n",
    "\n",
    "if os.path.exists(path_to_cropped):\n",
    "    shutil.rmtree(path_to_cropped)\n",
    "os.mkdir(path_to_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cea586a-7ce2-4481-933f-2394ac557e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cropped digit_img folder:  ./dataset/cropped/0\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/1\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/2\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/3\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/4\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/5\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/6\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/7\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/8\n",
      "Generating cropped digit_img folder:  ./dataset/cropped/9\n",
      "\n",
      "Succesfully Completed! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# cropping the dataset digit images\n",
    "\n",
    "crop_img_dirs = []\n",
    "digit_dict = {}\n",
    "\n",
    "for img_dir in img_dirs:\n",
    "    count = 1    \n",
    "    digit = (img_dir.split('/')[-1])\n",
    "    digit_dict[digit]=[]\n",
    "    \n",
    "    for entry in os.scandir(img_dir):\n",
    "        url = entry.path\n",
    "        \n",
    "        img_b64_transp = get_cv2_image_from_base64_string(None, url)\n",
    "        # print(img_b64_transp)\n",
    "        \n",
    "        reshape_img = get_img_reshape_by_cv2(img_b64_transp)\n",
    "        crop_digit = reshape_img\n",
    "        \n",
    "        \n",
    "        # crop_digit = crop_digit / 225.0\n",
    "        # print(crop_digit[0])\n",
    "        # plt.imshow(crop_digit[0])\n",
    "        # print(crop_digit[0].shape)\n",
    "        \n",
    "        if crop_digit[0] is not None:\n",
    "            crop_img_folder = path_to_cropped + digit\n",
    "            if not os.path.exists(crop_img_folder):\n",
    "                print(\"Generating cropped digit_img folder: \", crop_img_folder)\n",
    "                os.mkdir(crop_img_folder)\n",
    "                crop_img_dirs.append(crop_img_folder)\n",
    "            \n",
    "            crop_file_name = digit + \"_\" + str(count) + \".png\"\n",
    "            crop_file_path = crop_img_folder + \"/\" + crop_file_name\n",
    "            \n",
    "            \n",
    "            cv2.imwrite(crop_file_path, crop_digit[0])\n",
    "            digit_dict[digit].append(crop_file_path)\n",
    "            count+=1\n",
    "print(\"\\nSuccesfully Completed! ðŸ˜Š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7758eba-7413-4813-862e-dfb4a4e902a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/cropped/0\\\\0_1.png',\n",
       " './dataset/cropped/0\\\\0_10.png',\n",
       " './dataset/cropped/0\\\\0_11.png',\n",
       " './dataset/cropped/0\\\\0_12.png',\n",
       " './dataset/cropped/0\\\\0_13.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appending the file path of each of the digit image in the dictinary\n",
    "\n",
    "digit_dict = {}\n",
    "\n",
    "for img_dir in crop_img_dirs:\n",
    "    digit = img_dir.split(\"/\")[-1]\n",
    "    file_list = []\n",
    "    for entry in os.scandir(img_dir):\n",
    "        file_list.append(entry.path)\n",
    "    digit_dict[digit] = file_list\n",
    "\n",
    "digit_dict['0'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af5e79-9f17-49d3-9410-b4fa62a23d25",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Test, train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c73341-3491-470a-9313-64357ed86eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for digit, digit_img_path in digit_dict.items():\n",
    "    for training_img in digit_img_path:\n",
    "        img = cv2.imread(training_img)\n",
    "        grey = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2GRAY) # convert to gray\n",
    "        \n",
    "        \n",
    "        if grey is None:\n",
    "            continue\n",
    "            \n",
    "        # grey=grey/225.0\n",
    "        \n",
    "        # img_rshp = img.reshape(3, 28, 28)    # reshape img into 1D\n",
    "        X.append(grey)\n",
    "        y.append(int(digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb12b4fe-1853-4905-8265-9ef9acd0b032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, (28, 28))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dae2b7c-a2e6-4a80-bbcc-7205a1f357d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df199732-8620-4e18-aa1f-26eadb2d744c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d16ea79-1991-4f5e-b607-6956280ec635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X).reshape(len(X), 28, 28).astype(float)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fef57c1-0e53-4f09-9283-41953dfabda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54ff04-0c4e-4f4d-8f7e-b7a4cc00a4dd",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f41c5ca3-b606-4bdb-994f-e9c48792d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b35c3852-1e50-4009-8036-18500682e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d405f3b-2c78-4d47-ac80-fd2ba4454a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets, scale the image in a range of 0-1\n",
    "\n",
    "# X_train = X_train/255.0\n",
    "# X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875795ea-6ad0-4521-966c-dc23b0ba14dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8340ec20-ed7b-415d-82f2-4e1c2789e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    # tf.keras.layers.Dense(190, activation='relu'),\n",
    "    # tf.keras.layers.Dense(100, activation='sigmoid'),\n",
    "    # tf.keras.layers.Dense(50, activation='sigmoid'),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Dense(180, activation='relu'),\n",
    "    tf.keras.layers.Dense(170, activation='relu'),\n",
    "    tf.keras.layers.Dense(160, activation='relu'),\n",
    "    tf.keras.layers.Dense(150, activation='relu'),\n",
    "    tf.keras.layers.Dense(140, activation='relu'),\n",
    "    tf.keras.layers.Dense(130, activation='relu'),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dense(110, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(90, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(70, activation='relu'),\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    \n",
    "    \n",
    "    \n",
    "    # tf.keras.layers.Dense(40, activation='relu'),\n",
    "    # tf.keras.layers.Dense(30, activation='relu'),\n",
    "    # tf.keras.layers.Dense(20, activation='relu'),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b3c360c-a25e-429b-9482-e8fdfebf2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58745aa3-2fb6-4b44-839d-067885463cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98ba468-997d-495d-a0df-0c7979814798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "33/33 [==============================] - 1s 5ms/step - loss: 1.7847 - accuracy: 0.3754\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.7226 - accuracy: 0.7642\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8489\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.2439 - accuracy: 0.9365\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9596\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9808\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.2456 - accuracy: 0.9577\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.9548\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9836\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9711\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9865\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 0.9962\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9952\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0577 - accuracy: 0.9846\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x259a6cebd90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e68a73f8-24f9-4e7a-a31e-480272a1d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02148427627980709, 0.9961501359939575]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "047cb7d5-ecec-4e84-844e-e297fb8c6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "\n",
    "# for i in range(20):\n",
    "#     plt.subplot(5, 8, i+1)\n",
    "#     plt.imshow(X_train[i])\n",
    "#     plt.gray()\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     # plt.axis('off')\n",
    "#     plt.xlabel(y_train[i], color='#fff', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ae1b0-8399-4aa5-bb4e-5af36662c2c1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Lets, now check how well our model performs by giving an actuall image as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3d0d6d9-4a1f-4f89-b35a-dcf71d8404a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, 503, 3)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_img = get_cv2_image_from_base64_string(load_base64_img(), None)\n",
    "# n_img = get_cv2_image\"_from_base64_string(None, r'.\\dataset\\cropped\\2\\2_2.png')\n",
    "\n",
    "n_img.shape ## returns x, y, rbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "84361897-1ae9-42e3-b61d-720c86762d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x259a9dcb4f0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(n_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "279c23e7-8b4d-4656-a41d-2d1b2ef6d8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_img = get_img_reshape_by_cv2(n_img)\n",
    "crop_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1360c4a2-bf7e-4312-86d2-d44b54420245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x259a9ca9b10>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANTUlEQVR4nO3dX6hd9ZnG8ecxSRGTRvLHCSepmjTxwqKMGYIOjEwySIuKkPSmNMiQwcIp2EALFUY6SINDpQy244UQSFF6ZuhYKjFjKBNSJ9bYuSmeiBNjTJs0JjThJEeTi6QRjJp3Ls6KcxrP+u3j/rf2yfv9wGHvvd699n7Z5Mn689t7/RwRAnD1u6bpBgD0B2EHkiDsQBKEHUiCsANJzO7nm9nm1D/QYxHhqZZ3tGW3fa/t39k+YvvRTl4LQG+53XF227Mk/V7SlyWdkPSapI0RcbCwDlt2oMd6sWW/U9KRiDgaERcl/VzS+g5eD0APdRL2ZZL+OOnxiWrZn7E9bHvU9mgH7wWgQz0/QRcR2yRtk9iNB5rUyZb9pKQbJz3+QrUMwADqJOyvSbrF9grbn5P0dUk7u9MWgG5rezc+Ij6yvVnSbkmzJD0bEW91rTMAXdX20Ftbb8YxO9BzPflSDYCZg7ADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk2p6yOZv77ruvtjY8PFxcd+nSpcX6ddddV6xfvHixWG/SNdeUtxeHDh2qrR08eLC47rvvvlusnzlzplg/evRobe3w4cPFdc+dO1esz0Qdhd32MUnnJX0s6aOIWNONpgB0Xze27H8XEe914XUA9BDH7EASnYY9JP3K9j7bUx642h62PWp7tMP3AtCBTnfj746Ik7b/QtJLtg9FxKuTnxAR2yRtkyTb0eH7AWhTR1v2iDhZ3Y5L2iHpzm40BaD72g677bm2P3/5vqSvSDrQrcYAdJcj2tuztv1FTWzNpYnDgf+IiB+0WGfG7sbPnz+/tjZ37tziuu+//36xPsjj6K3Mnl0+Ely0aFFtrdX3D1auXFmsr1q1qlhfsWJFsV7y+OOPF+tHjhxp+7V7LSI81fK2j9kj4qikv2y7IwB9xdAbkARhB5Ig7EAShB1IgrADSbQ99NbWm83goTcMHnvKEaZPlIYFR0ZGiuvu2LGjWH/++eeL9SbVDb2xZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJLiUNGasVj+v3bBhQ23t2muvLa77yiuvtNHRYGPLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OgTVr1qxiffPmzcX6gw8+2FZNaj1d9EzElh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHT1VGitvNWXzww8/XKzfeuutxfratWtraxcuXCiuezVquWW3/aztcdsHJi1baPsl24er2wW9bRNAp6azG/9TSfdesexRSXsi4hZJe6rHAAZYy7BHxKuSzl6xeL2ky/PnjEja0N22AHRbu8fsSyJirLp/StKSuifaHpY03Ob7AOiSjk/QRUSUJmyMiG2StklM7Ag0qd2ht9O2hySpuh3vXksAeqHdsO+UtKm6v0nSi91pB0CvtJyf3fZzktZJWizptKTvS/pPSb+QdJOk45K+FhFXnsSb6rXYjb/K3HXXXcX6Qw89VFu77bbbiuu+/PLLxfpTTz1VrJ85c6ZYv1rVzc/e8pg9IjbWlO7pqCMAfcXXZYEkCDuQBGEHkiDsQBKEHUiCn7iiI62G3tatW1dbW7x4cXHd+fPnF+t79+4t1vfs2VNbazXkfDViyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSbT8iWtX34yfuKZjT/lrS0nS7Nnlr3msXr26WN+yZUuxPjo6Wlt7+umni+uOj8/c67HU/cSVLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4O2ashQsXFuubN2+urd1www3FdR955JFi/YMPPijWm8Q4O5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTg7rlpz5syprW3fvr247tatW4v1Xbt2tdVTP7Q9zm77Wdvjtg9MWrbF9knbb1R/93ezWQDdN53d+J9KuneK5f8aEXdUf//V3bYAdFvLsEfEq5LO9qEXAD3UyQm6zbb3V7v5C+qeZHvY9qjt+guCAei5dsO+VdJKSXdIGpP0o7onRsS2iFgTEWvafC8AXdBW2CPidER8HBGXJP1E0p3dbQtAt7UVdttDkx5+VdKBuucCGAwt52e3/ZykdZIW2z4h6fuS1tm+Q1JIOibpm71rEWjPhx9+WFsbGRkprvvAAw8U64M8zl6nZdgjYuMUi5/pQS8AeoivywJJEHYgCcIOJEHYgSQIO5AEP3FFSsuWLSvW9+/fX6wvWrSom+10FZeSBpIj7EAShB1IgrADSRB2IAnCDiRB2IEkWv7qDbgalX7+KrWeDnomYssOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzo6Ubr/99mJ97969feqkf9iyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLNXZs2aVaxfunSpttbPa+/j/9lTXh79E/PmzautPfnkk8V1H3vssbZ6GmQtt+y2b7T9a9sHbb9l+9vV8oW2X7J9uLpd0Pt2AbRrOrvxH0n6bkR8SdJfS/qW7S9JelTSnoi4RdKe6jGAAdUy7BExFhGvV/fPS3pb0jJJ6yWNVE8bkbShRz0C6ILPdMxue7mk1ZJ+K2lJRIxVpVOSltSsMyxpuIMeAXTBtM/G254nabuk70TEucm1mDhDNeVZqojYFhFrImJNR50C6Mi0wm57jiaC/rOIeKFafNr2UFUfkjTemxYBdEPL3XhPjG88I+ntiPjxpNJOSZsk/bC6fbEnHfZJq6GYs2fP1tZ27drV9rqSdP78+WL9woULxXqTQ39z5swp1q+//vra2vz584vrLl++vFhfu3Ztsb569era2hNPPFFcd/fu3cX6TDSdY/a/kfT3kt60/Ua17HuaCPkvbH9D0nFJX+tJhwC6omXYI+J/JNV9e+Ge7rYDoFf4uiyQBGEHkiDsQBKEHUiCsANJuJ9jtLYH9regN910U7F+zz31Aw+rVq0qrrt06dJi/eabby7Wly1bVqwPDQ0V651oNbXxsWPHivXjx4/X1t55553iuqdOnSrWDx06VKzv27evtjY2NlZbk2b2z5YjYsrRM7bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+zAVYZxdiA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiiZdht32j717YP2n7L9rer5Vtsn7T9RvV3f+/bBdCulhevsD0kaSgiXrf9eUn7JG3QxHzsf4qIJ6f9Zly8Aui5uotXTGd+9jFJY9X987bfllSeogTAwPlMx+y2l0taLem31aLNtvfbftb2gpp1hm2P2h7trFUAnZj2Nehsz5O0V9IPIuIF20skvScpJP2zJnb1H2rxGuzGAz1Wtxs/rbDbniPpl5J2R8SPp6gvl/TLiLitxesQdqDH2r7gpG1LekbS25ODXp24u+yrkg502iSA3pnO2fi7Jf1G0puSLlWLvydpo6Q7NLEbf0zSN6uTeaXXYssO9FhHu/HdQtiB3uO68UByhB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSRaXnCyy96TdHzS48XVskE0qL0Nal8SvbWrm73dXFfo6+/ZP/Xm9mhErGmsgYJB7W1Q+5LorV396o3deCAJwg4k0XTYtzX8/iWD2tug9iXRW7v60lujx+wA+qfpLTuAPiHsQBKNhN32vbZ/Z/uI7Ueb6KGO7WO236ymoW50frpqDr1x2wcmLVto+yXbh6vbKefYa6i3gZjGuzDNeKOfXdPTn/f9mN32LEm/l/RlSSckvSZpY0Qc7GsjNWwfk7QmIhr/Aobtv5X0J0n/dnlqLdv/IulsRPyw+o9yQUT844D0tkWfcRrvHvVWN834P6jBz66b05+3o4kt+52SjkTE0Yi4KOnnktY30MfAi4hXJZ29YvF6SSPV/RFN/GPpu5reBkJEjEXE69X985IuTzPe6GdX6Ksvmgj7Mkl/nPT4hAZrvveQ9Cvb+2wPN93MFJZMmmbrlKQlTTYzhZbTePfTFdOMD8xn1870553iBN2n3R0RfyXpPknfqnZXB1JMHIMN0tjpVkkrNTEH4JikHzXZTDXN+HZJ34mIc5NrTX52U/TVl8+tibCflHTjpMdfqJYNhIg4Wd2OS9qhicOOQXL68gy61e14w/18IiJOR8THEXFJ0k/U4GdXTTO+XdLPIuKFanHjn91UffXrc2si7K9JusX2Ctufk/R1STsb6ONTbM+tTpzI9lxJX9HgTUW9U9Km6v4mSS822MufGZRpvOumGVfDn13j059HRN//JN2viTPyf5D0T030UNPXFyX9b/X3VtO9SXpOE7t1H2ri3MY3JC2StEfSYUn/LWnhAPX275qY2nu/JoI11FBvd2tiF32/pDeqv/ub/uwKffXlc+PrskASnKADkiDsQBKEHUiCsANJEHYgCcIOJEHYgST+D55OLusV2D1GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(crop_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bba6345c-7066-4eb5-b6c7-d494397b7f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(crop_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "afec7a20-913a-48a8-abe5-0be3309311b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b03b0a80-b7d8-417e-b4c0-30e2cfb5e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pred = []\n",
    "\n",
    "\n",
    "for i in range(len(crop_img)):\n",
    "    \n",
    "    if len(crop_img) > 1:\n",
    "        pred = model.predict(crop_img[i])\n",
    "    else:\n",
    "        pred = model.predict(crop_img)\n",
    "        \n",
    "    num_pred.append(np.argmax(pred))\n",
    "num_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "222ca869-affa-421d-8661-817da4a6a4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3], 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pred, len(num_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a740e4-102e-468c-a736-10f11bbeff66",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a66f7ab-8fc0-4bf6-8247-bd51e2003148",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
